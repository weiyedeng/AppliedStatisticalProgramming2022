controldiff_mean <- mean(boston$numberim.post[boston$treatment == 0], na.rm = T) - mean(boston$numberim.pre[boston$treatment == 0], na.rm = T)
treatdiff_mean ##0.08128342 became less likely to be proimmigrant over the course, 0.08 increase
controldiff_mean ## -0.1873708 control became more likely to be proimmigrant over the course, increased by 0.19
##Differences in Differences
treatdiff_mean - controldiff_mean
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
# Change eval=FALSE in the code block. Install packages as appropriate.
#install.packages("fivethirtyeight")
rm(list=ls())
library(fivethirtyeight)
library(tidyverse)
library(RColorBrewer)
library(webshot)
library(htmlwidgets)
library(tidytext)
# URL to the data that you've used.
polls <- read.csv("/Users/rexdeng/Dropbox/mine/academics_career/WashU/Classes/202122Spring-Stat programming Jacob/PS/ASP2022_PS/president_primary_polls_feb2020.csv", na.strings = "")
`%notin%` <- Negate(`%in%`)
#0 Reset the data frame when running this chunk
Endorsements <- endorsements_2020 # from the fiverthirtyeight package
#1.1 Rename
Endorsements <- Endorsements %>%
rename(candidate_name = endorsee)
#1.2 as.tibble
Endorsements <- as_tibble(Endorsements)
#1.3 filter and select
polls <- polls %>%
filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders","Elizabeth Warren",
"Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>%
dplyr::select(candidate_name, sample_size, start_date, party, pct)
#1.4 Make names the same
Endorsements <- Endorsements %>%
mutate(candidate_name = ifelse(candidate_name == "Joe Biden", "Joseph R. Biden Jr.",
ifelse(candidate_name == "Bernie Sanders", "Bernard Sanders", candidate_name)))
## check which candidate names in polls are still not in Endorsements
unique(polls$candidate_name)[unique(polls$candidate_name) %notin% unique(Endorsements$candidate_name)] ### Make sense as there is no record in polls
#1.5 Join
polls_Endorse_joined <- left_join(polls, Endorsements, by = "candidate_name")
#1.6 The number of endorsements for each of the five candidates
## The joined dataset is not as useful as the original endorsement dataset as the joined one contains duplication of endorsers due to multiple matches
Endorsements_sum <- Endorsements %>%
filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders","Elizabeth Warren",
"Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>%
group_by(candidate_name) %>%
summarise(n_endorsements = sum(!is.na(endorser)))
#1.7 ggplot
p <- Endorsements_sum %>%
ggplot() +
geom_bar(aes(x = candidate_name, y=n_endorsements), stat = "identity")
#1.8, 1.9 add a theme and labs and so on
p <- p +
labs(title = "Number of Endorsements by Selected Democratic Candidates",
x = "Candidate Name",
y = "Number of Endorsements") +
theme_dark()
p
ggsave("Q1.png", width = 10)
tweets <- read_csv(trump_tweets_url)
# Change eval=FALSE in the code block. Install packages as appropriate.
library(tidyverse)
library(tm)
library(lubridate)
library(wordcloud)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
# 2.1 Separate date and time
tweets$date <- sapply(strsplit(tweets$created_at, " "), `[[`, 1)
tweets$time <- sapply(strsplit(tweets$created_at, " "), `[[`, 2)
tweets$date <- as.Date(tweets$date, format="%m/%d/%Y")
range(tweets$date)
# 2.2
top5_retweet_count <- tweets %>%
filter(is_retweet == F) %>%
slice_max(retweet_count, n=5)
#2.3 Corpus
Corpus <- VCorpus(VectorSource(tweets$text))
writeLines(head(strwrap(Corpus[[1]]), 10)) ## Check the tweets
#2.4 Remove
## Create a function called "addspace" that finds a user specified pattern and substitutes the pattern with a space.
addspace <- content_transformer(function(x, pattern) {
return(gsub(pattern, " ", x))
})
## For words connected by "-", replace it with a white space so that it won't be connected after space removal
Corpus <- tm_map(Corpus, addspace, "-")
## Remove patterns
removepattern <- content_transformer(function(x, pattern) {
return(gsub(pattern, "", x))
})
Corpus <- tm_map(Corpus, removepattern, "?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)")
Corpus <- tm_map(Corpus, removepattern, "‘")
Corpus <- tm_map(Corpus, removepattern, "’")
## Remove others
Corpus <- tm_map(Corpus, stripWhitespace)
Corpus <- tm_map(Corpus, removePunctuation)
Corpus <- tm_map(Corpus, removeNumbers)
Corpus <- tm_map(Corpus, removeWords, stopwords("english"))
Corpus <- tm_map(Corpus,content_transformer(tolower))
writeLines(head(strwrap(Corpus[[1]]), 10)) ## Check the tweets again
#2.5 wordcloud
pal = brewer.pal(9,"BuGn")
wc <- wordcloud(Corpus, min.freq = 3, random.order = T, random.color =T, max.words = 50, colors = pal)
wc
#2.6 dtm
DTM <- DocumentTermMatrix(Corpus, control = list(weighting = weightTfIdf))
df_DTM <- tidy(DTM)
df_DTM_top50 <- df_DTM %>%
slice_max(count, n=50)
View(Corpus)
View(top5_retweet_count)
View(tweets)
## Do the results support the hypothesis?
## Solution:
cct <- read.csv("/Users/rexdeng/Dropbox/WashU MTE/00_QPM_U_Shared/03_Labs/Lab_08/data/progresa.csv")
View(cct)
unique(cct$treatment)
cct$t2000[cct$treatment == 1]
mean(cct$t2000[cct$treatment == 1])
ate_t2000 <- mean(cct$t2000[cct$treatment == 1]) - mean(cct$t2000[cct$treatment == 0])
## method 2
tapply(cct$t2000, cct$treatment, mean, na.rm=T)
ols1 <- lm(t2000 ~ treatment, data = cct)
?lm
ols1 <- lm(t2000 ~ treatment, data = cct)
View(ols1)
summary(ols1)
## method 2
tapply(cct$t2000, cct$treatment, mean, na.rm=T)
ols2 <- lm(t2000 ~ treatment + avgpoverty + pobtot1994 +
votos1994 + pri1994 + pan1994 + prd1994, data = cct)
summary(ols2)
summary(ols2)
ols5_1 <- lm(t2000r ~ treatment + avgpoverty + log(pobtot1994) +
t1994r + pri1994v + pan1994v + prd1994v, data = cct)
summary(ols5_1)
ols5_2 <- lm(pri2000v ~ treatment + avgpoverty + log(pobtot1994) +
t1994r + pri1994v + pan1994v + prd1994v, data = cct)
summary(ols5_2)
hist(cct$pobtot1994)
hist(log(cct$pobtot1994))
ols5_1 <- lm(t2000r ~ treatment + avgpoverty + log(pobtot1994) +
t1994r + pri1994v + pan1994v + prd1994v, data = cct)
summary(ols5_1)
ols5_2 <- lm(pri2000v ~ treatment + avgpoverty + log(pobtot1994) +
t1994r + pri1994v + pan1994v + prd1994v, data = cct)
summary(ols5_2)
hist(cct$pobtot1994)
hist(log(cct$pobtot1994))
cct <- read.csv("/Users/rexdeng/Dropbox/WashU MTE/00_QPM_U_Shared/03_Labs/Lab_08/data/progresa.csv")
View(cct)
cct$t2000[cct$treatment==1]
ate <- mean(cct$t2000[cct$treatment==1]) - mean(cct$t2000[cct$treatment==0])
tapply(cct$t2000, cct$treatment, mean, na.rm = T)
?lm
## now it's regression
ols1 <- lm(t2000 ~ treatment, data = cct)
View(ols1)
summary(ols1)
## method 2
tapply(cct$t2000, cct$treatment, mean, na.rm = T)
ols2 <- lm(t2000 ~ treatment + avgpoverty + pobtot1994 +
votos1994 + pri1994 + pan1994 + prd1994, data = cct)
summary(ols2)
coef(ols2)
coef(ols2)[2]
coef(ols2)["treatment"]
ols5_1 <-lm(t2000r ~ treatment + avgpoverty + log(pobtot1994) +
t1994r + pri1994v + pan1994v + prd1994v, data = cct)
summary(ols5_1)
ols5_2 <-lm(pri2000v ~ treatment + avgpoverty + log(pobtot1994) +
t1994r + pri1994v + pan1994v + prd1994v, data = cct)
summary(ols5_2)
hist(cct$pobtot1994)
hist(log(cct$pobtot1994))
library(tidyverse)
library(tidyverse)
library(plyr)
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
rm(list = ls())
library(plyr)
library(tidyverse)
library(stargazer)
library(AER)
load("/Users/rexdeng/Dropbox/mine/academics_career/WashU/Classes/202122Spring-Causal Chris/PS/lee.RData")
d <- d %>%
filter(party == 100) %>% ## filter democrats
mutate(share_t = origvote / totvote, ## get vote share of total votes
share_toptwo = origvote / (highestvote + sechighestvote), ## Get vote share of top two candidates' votes
win_current = ifelse(share_toptwo > 0.5, 1, 0)) %>%
## A candidate wins more than 50% votes cast for the top two candidates -> highest vote
group_by(state, distid) %>%
mutate(margin_tm1 = lag(share_toptwo, order_by = yearel),
incumbent = lag(win_current, order_by = yearel)) %>%
ungroup()
## test
d %>%
ggplot() +
geom_point(aes(x=margin_tm1, y=incumbent)) +
labs(title="Test of Variable Creation")
library(rdd)
library(gridExtra)
d <- d %>%
mutate(margin_tm1_sq = margin_tm1^2)
mod_b1 <- lm(share_t ~ incumbent + margin_tm1, data = d)
mod_b2 <- lm(share_t ~ incumbent * margin_tm1, data = d)
mod_b3 <- lm(share_t ~ incumbent * margin_tm1 + incumbent * margin_tm1_sq, data = d)
stargazer(list(mod_b1, mod_b2, mod_b3), type = "text")
plot_b1 <- d %>%
filter(!is.na(incumbent)) %>%
mutate(b1_fitted = mod_b1$fitted.values) %>%
ggplot() +
geom_point(aes(x=margin_tm1, y=share_t)) +
geom_line(aes(x=margin_tm1, y=b1_fitted, color=as.factor(incumbent))) +
xlim(0.49,0.51) +
labs(title="Constant Slope")
plot_b2 <- d %>%
filter(!is.na(incumbent)) %>%
mutate(b2_fitted = mod_b2$fitted.values) %>%
ggplot() +
geom_point(aes(x=margin_tm1, y=share_t)) +
geom_line(aes(x=margin_tm1, y=b2_fitted, color=as.factor(incumbent))) +
labs(title="Different Slopes")
plot_b3 <- d %>%
filter(!is.na(incumbent)) %>%
mutate(b3_fitted = mod_b3$fitted.values) %>%
ggplot() +
geom_point(aes(x=margin_tm1, y=share_t)) +
geom_line(aes(x=margin_tm1, y=b3_fitted, color=as.factor(incumbent))) +
labs(title="Different Slopes with Quadratic Adjustfications")
grid.arrange(plot_b1, plot_b2, plot_b3, nrow = 3)
coefs <- adply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coefs = coef(mod_d2)[2]
lwr = confint(mod_d2)[2,1]
upr = confint(mod_d2)[2,2]
band = bd * 2
cbind(coefs, lwr, upr, band)
})
coefs <- adply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coefs = coef(mod_d2)[2]
lwr = confint(mod_d2)[2,1]
upr = confint(mod_d2)[2,2]
band = bd * 2
})
bds <- seq(0.01,0.5,0.01)
coefs <- adply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coefs = coef(mod_d2)[2]
lwr = confint(mod_d2)[2,1]
upr = confint(mod_d2)[2,2]
band = bd * 2
})
？a_ply
?a_ply
?adply
rbind(coef, lwr, upr, band)
(
bds <- seq(0.01,0.5,0.01)
coefs <- sapply(bds, function(bd) {
coefs <- sapply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coef = coef(mod_d2)[2]
lwr = confint(mod_d2)[2,1]
upr = confint(mod_d2)[2,2]
band = bd * 2
rbind(coef, lwr, upr, band)
})
View(coefs)
coefs <- sapply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coef <- coef(mod_d2)[2]
lwr <- confint(mod_d2)[2,1]
upr <- confint(mod_d2)[2,2]
band <- bd * 2
cbind(coef, lwr, upr, band)
})
bds <- seq(0.01,0.5,0.01)
coefs <- sapply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coef <- coef(mod_d2)[2]
lwr <- confint(mod_d2)[2,1]
upr <- confint(mod_d2)[2,2]
band <- bd * 2
cbind(coef, lwr, upr, band)
})
View(coefs)
coefs <- lapply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coef <- coef(mod_d2)[2]
lwr <- confint(mod_d2)[2,1]
upr <- confint(mod_d2)[2,2]
band <- bd * 2
list(coef, lwr, upr, band)
})
View(coefs)
coefs <- lapply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coef <- coef(mod_d2)[2]
lwr <- confint(mod_d2)[2,1]
upr <- confint(mod_d2)[2,2]
band <- bd * 2
c(coef, lwr, upr, band)
})
View(coefs)
?apply
?lapply
?dapply
do.call(rbind, coefs)
as.data.frame(coefs)
as.data.frame(coefs)
coefs <- as.data.frame(do.call(rbind, coefs))
View(coefs)
names(coefs) <- c("coef", "lwr", "upr", "band")
View(coefs)
coefs %>%
ggplot(coefs, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
# geom_hline(yintercept = 0, linetype = 'dashed') +
# geom_hline(yintercept = 3, linetype = 'dashed') +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
ggplot(coefs, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
# geom_hline(yintercept = 0, linetype = 'dashed') +
# geom_hline(yintercept = 3, linetype = 'dashed') +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
ggplot(coefs, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
geom_hline(yintercept = 0, linetype = 'dashed') +
geom_hline(yintercept = 3, linetype = 'dashed') +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
ggplot(coefs, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
ylim(-1,1) +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
ggplot(coefs, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
ylim(-1,1) +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
ggplot(coefs, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
ggplot(d) %>%
geom_density(aes(x=margin_tm1))
ggplot(d) +
geom_density(aes(x=margin_tm1))
ggplot(d) +
geom_density(aes(x=margin_tm1)) +
geom_vline(aes(y=0.5))
ggplot(d) +
geom_density(aes(x=margin_tm1)) +
geom_vline(xintercept = 0.5)
?geom_vline
ggplot(d) +
geom_density(aes(x=margin_tm1)) +
geom_vline(aesxintercept = 0.5, shape = "dashed"))
ggplot(d) +
geom_density(aes(x=margin_tm1)) +
geom_vline(aes(xintercept = 0.5, shape = "dashed"))
ggplot(d) +
geom_density(aes(x=margin_tm1)) +
geom_vline(aes(xintercept = 0.5, linetype = "dashed"))
ggplot(d) +
geom_density(aes(x=margin_tm1)) +
geom_vline(xintercept = 0.5, linetype = "dashed")
?DCdensity
DCdensity(d$margin_tm1, 0.5)
density_test <- DCdensity(d$margin_tm1, 0.5)
DCdensity(d$margin_tm1, 0.5， est.out=T)
DCdensity(d$margin_tm1, 0.5, est.out=T)
DCdensity(d$margin_tm1, 0.5, ext.out=T)
density_test <- DCdensity(d$margin_tm1, 0.5, ext.out=T)
View(density_test)
density_test$p
density_test <- DCdensity(d$margin_tm1, 0.5, bin = 0.01, ext.out=T)
density_test$p
?stargazer
stargazer(list(mod_b1, mod_b2, mod_b3), type = "text",
keep.stat = c("n", "adj.rsq"))
ggplot(d, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
bds <- seq(0.01,0.5,0.01)
coefs <- lapply(bds, function(bd) {
mod_d2 <- lm(share_t ~ incumbent * margin_tm1, data = d %>% filter(margin_tm1 > (0.5 - bd) & margin_tm1 < (0.5 + bd)))
coef <- coef(mod_d2)[2]
lwr <- confint(mod_d2)[2,1]
upr <- confint(mod_d2)[2,2]
band <- bd * 2
c(coef, lwr, upr, band)
})
coefs <- as.data.frame(do.call(rbind, coefs))
names(coefs) <- c("coef", "lwr", "upr", "band")
ggplot(d, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
ggplot(coefs, aes(x = band, y = coef)) +
geom_point() +
geom_errorbar(aes(x = band, ymin = lwr, ymax = upr), width = 0) +
labs(x = 'Bandwidth Size', y = 'Treatment Coefficient Estimate')
blackturnout <- read.csv("/Users/rexdeng/Dropbox/WashU MTE/2122S_QPMUG/03_Labs/Lab_09/data/blackturnout.csv")
View(blackturnout)
unique(blackturnout$year)
unique(blackturnout$state)
length(unique(blackturnout$state))
?boxplot
boxplot(turnout ~ candidate, data = blackturnout)
plot(blackturnout$candidate, blackturnout$turnout)
boxplot(turnout ~ candidate, data = blackturnout)
ols3 <- lm(turnout ~ candidate, data = blackturnout)
summary(ols3)
ols3_2 <- lm(turnout ~ candidate + CVAP, data = blackturnout)
summary(ols3_2)
View(blackturnout)
## Load libraries and set working directory
library(devtools)
library(roxygen2)
setwd("/Users/rexdeng/Dropbox/mine/academics_career/WashU/Classes/202122Spring-Stat programming Jacob/PS/ASP2022_PS/integrateItIntro") #This will need to be changed to match your directory
## This can be run many times as the code is updates
current.code <- as.package("integrateIt")
check(current.code)
load_all(current.code)
document(current.code)
## Let's look at a function
?tolTest
?integrateIt
## Let's look at a function
?tolTest
?integrateIt
## Load libraries and set working directory
library(devtools)
library(roxygen2)
setwd("/Users/rexdeng/Dropbox/mine/academics_career/WashU/Classes/202122Spring-Stat programming Jacob/PS/ASP2022_PS/integrateItIntro") #This will need to be changed to match your directory
## This is run once when the package structure is first created
## This can be run many times as the code is updates
current.code <- as.package("integrateIt")
check(current.code)
load_all(current.code)
document(current.code)
## Let's look at a function
?tolTest
## Let's look at a function
?integrateIt
## Let's try it out
fx <- function(x) {
return(x^3 + x^2 + 1)
}
x <- seq(0,3,0.1)
ends <- c(0,3)
Tr <- integrateIt(x=x,fun=fx,ends=ends,Rule="Trapezoid")
Si <- integrateIt(x=x,fun=fx,ends=ends,Rule="Simpson")
integrate(fx,0,3) ## Compared with the result calculated by integrate()
Tr
class(Tr$Integrated)
show(Tr[[1]])
show(Si[[1]])
View(Si)
## Load libraries and set working directory
library(devtools)
library(roxygen2)
setwd("/Users/rexdeng/Dropbox/mine/academics_career/WashU/Classes/202122Spring-Stat programming Jacob/PS/ASP2022_PS/integrateItIntro") #This will need to be changed to match your directory
## This can be run many times as the code is updates
current.code <- as.package("integrateIt")
check(current.code)
load_all(current.code)
document(current.code)
## Let's look at a function
?integrateIt
?tolTest
install.packages("testthat")
install.packages("testthat")
library(testthat)
expect_that(10, equals(10))
??expect_that
library(testthat)
install.packages("rlang")
install.packages("rlang")
library(testthat)
expect_that(10, equals(10))
expect_vector
?expect_vector
?expect_that
?test_that
test_that("trigonometric functions match identities", {
expect_equal(sin(pi / 4), 1 / sqrt(2))
expect_equal(cos(pi / 4), 1 / sqrt(2))
expect_equal(tan(pi / 4), 1)
})
test_that("trigonometric functions match identities", {
expect_equal(sin(pi / 4), 1 / sqrt(2))
expect_equal(cos(pi / 4), 2 / sqrt(2))
expect_equal(tan(pi / 4), 1)
})
?NA_integer_
test_check("squarePack")
library(testthat)
library(testthat)
test_check("squarePack")
context("Adding squares")
test_that("squares add correctly", {
expect_that(addSquares(2,3),
equals(new("Squares", square=(13), x = 2, y = 3)))
expect_that(addSquares(2,2),
equals(new("Squares", square=(8), x = 2, y = 2)))
})
